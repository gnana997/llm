# Example configuration file for LLaMA CLI
# Copy this to ~/.llmrc or ./.llmrc and customize

# Model configuration
model:
  path: /path/to/your/model.gguf
  gpu_layers: 32
  
# Generation defaults
generation:
  temperature: 0.8
  top_k: 40
  top_p: 0.95
  repeat_penalty: 1.1
  
# Performance settings
performance:
  threads: 8
  batch_size: 512
  context_size: 4096
  
# Server configuration (for 'llm server' command)
server:
  host: 127.0.0.1
  port: 8080
  api_key: your-api-key-here
  
# Logging
logging:
  level: info
  file: llm.log